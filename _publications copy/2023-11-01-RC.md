---
title: "Assessing the Quality of Uncertainty Calibration"
collection: publications
permalink: /publication/2023-11-01-RC
excerpt: 'This poster introduces RC-Index metric to evaluate uncertainty estimates.'
date: 2023-11-01
venue: 'American Medical Informatics Association Annual Symposium 2023'
paperurl: 'https://katherinebrown539.github.io/files/Assessing%20the%20Quality%20of_Brown.pdf'
citation: 'Brown KE, Talbert S, Talbert DA. &quot;Assessing the Quality of Uncertainty Calibration.&quot; <i>Proceedings of the American Medical Informatics Association Annual Symposium</i>, 2023.'
---
Uncertainty quantification provides an indicator of potential incorrectness in machine learning models. Formal, statistical analysis of the efficacy of uncertainty requires converting the rejection-classification curve to a numeric representation, which can be accomplished by taking the area under the rejection-classification plot (AURCP). Since AURCP does not have a standard starting point, this limits their usefulness beyond comparison to other AURCP values. This poster describes and demonstrates a novel adaptation of AURCP that addresses this limitation

[Download paper here](https://katherinebrown539.github.io/files/Assessing%20the%20Quality%20of_Brown.pdf)

[Download poster here](https://katherinebrown539.github.io/files/P01%20Brown.pdf)

Recommended citation: Brown KE, Talbert S, Talbert DA. “Assessing the Quality of Uncertainty Calibration.” <i>Proceedings of the American Medical Informatics Association Annual Symposium</i>, 2023.
